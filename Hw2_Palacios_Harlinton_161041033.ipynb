{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                  <center> CSE555 â€“ Deep Learning (Spring 2018) </center>\n",
    "###                                               <center> Homework 2 </center>\n",
    "###                                    <center>   Harlinton Palacios Mosquera </center>\n",
    "                                   \n",
    "###                                       <center>  ID: 161041033  </center>                                        \n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import os\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_frame(joints, fname, simplified=False):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d', xlabel='x', ylabel='y', zlabel='z', )\n",
    "    ax = fig.gca(projection='3d', xlim=[16.4, 16.9], ylim=[3.9, 4.4], zlim=[383, 383.5], xlabel='x', ylabel='y', zlabel='z', )\n",
    "    ax.view_init(azim=60)\n",
    "    joints_lines = [[20, 1, 2, 1, 8 , 10, 2, 9 , 11, 3, 4, 7, 7, 5 , 6 , 14, 15, 16, 17],\n",
    "                    [3 , 3, 3, 8, 10, 12, 9, 11, 13, 4, 7, 5, 6, 14, 15, 16, 17, 18, 19]]\n",
    "\n",
    "\n",
    "    if(simplified):\n",
    "        joints = joints.reshape((20, 3))\n",
    "\n",
    "    [ax.scatter(j[0], j[2]/4, 400-j[1]) for j in joints]\n",
    "\n",
    "    for i in range(19):\n",
    "        c1 = joints_lines[0][i] - 1\n",
    "        c2 = joints_lines[1][i] - 1\n",
    "        p1x, p1y, p1z = joints[c1]\n",
    "        p2x, p2y, p2z = joints[c2]\n",
    "        x = np.array([p1x, p2x])\n",
    "        y = 400-np.array([p1y, p2y])\n",
    "        z = np.array([p1z, p2z])/4\n",
    "        ax.plot(x, z, y)\n",
    "    #plt.show()\n",
    "    fig.savefig(fname)\n",
    "    plt.cla()\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_action(action, save_dir, simplified=False):\n",
    "    for i, joints in enumerate(action):\n",
    "        save_frame(joints, \"{0}/{1}.png\".format(save_dir, i), simplified)\n",
    "\n",
    "def read_action(fname, simplified=False, normed=False):\n",
    "    data = np.loadtxt(fname)[:,:3]\n",
    "    if(normed):\n",
    "        data[:,0] = 1e-4 +data[:,0] / np.linalg.norm(data[:,0])\n",
    "        data[:,1] = 1e-4 +data[:,1] / np.linalg.norm(data[:,1])\n",
    "        data[:,2] = 1e-4 +data[:,2] / np.linalg.norm(data[:,2])       \n",
    "    action = np.split(data, len(data)/20)\n",
    "    if(simplified):\n",
    "        action = np.array([joints.reshape((20*3)) for joints in action])\n",
    "    return action\n",
    "\n",
    "def read_actions(data_dir, simplified=False, normed=False):\n",
    "    return [read_action(os.path.join(data_dir, f), simplified, normed) for f in os.listdir(data_dir)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section prepared the data set using a vector (joints_lines) which I used to connect the skeletal extremities. also the function is created to save the frame that will generate the nueronal network.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Xiaoqiang_Li/publication/312642332/figure/fig2/AS:541682212052992@1506158511009/Skeleton-of-MSR-Action-3D.png\" alt=\"Drawing\" style=\"width: 100px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x00000211E77B5B00>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x00000211E3BBC048>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "batch:  0    loss:  1.82675    speed:  201.4594907968889  batches / s\n",
      "batch:  100    loss:  0.161753    speed:  5.454073450251663  batches / s\n",
      "batch:  200    loss:  0.461087    speed:  5.573392707322988  batches / s\n",
      "batch:  300    loss:  0.139019    speed:  5.521544990609644  batches / s\n",
      "batch:  400    loss:  0.198992    speed:  5.524571768383797  batches / s\n",
      "batch:  500    loss:  0.306895    speed:  5.448751258552127  batches / s\n",
      "batch:  600    loss:  0.336003    speed:  5.355743866693985  batches / s\n",
      "batch:  700    loss:  0.113127    speed:  5.091218333466654  batches / s\n",
      "batch:  800    loss:  0.119345    speed:  5.219747189651741  batches / s\n",
      "batch:  900    loss:  0.157287    speed:  5.243490997788517  batches / s\n",
      "batch:  1000    loss:  0.189803    speed:  5.258978192407551  batches / s\n",
      "batch:  1100    loss:  0.105675    speed:  5.171756645031409  batches / s\n",
      "batch:  1200    loss:  0.133948    speed:  4.873671200211879  batches / s\n",
      "batch:  1300    loss:  0.128014    speed:  4.870456297557969  batches / s\n",
      "batch:  1400    loss:  0.119592    speed:  3.9154687117212843  batches / s\n",
      "batch:  1500    loss:  0.0890403    speed:  3.7000931087320237  batches / s\n",
      "batch:  1600    loss:  0.105789    speed:  4.874614795040907  batches / s\n",
      "batch:  1700    loss:  0.0997852    speed:  4.896144873546962  batches / s\n",
      "batch:  1800    loss:  0.0904473    speed:  4.8712086663507925  batches / s\n",
      "batch:  1900    loss:  0.10508    speed:  4.940628760050724  batches / s\n",
      "batch:  2000    loss:  0.0907992    speed:  4.710842299327329  batches / s\n",
      "batch:  2100    loss:  0.094392    speed:  4.641786619675211  batches / s\n",
      "batch:  2200    loss:  0.0956098    speed:  4.140419790170955  batches / s\n",
      "batch:  2300    loss:  0.0926782    speed:  3.732092050352541  batches / s\n",
      "batch:  2400    loss:  0.0943668    speed:  4.428345467799198  batches / s\n",
      "batch:  2500    loss:  0.0923147    speed:  4.402972397715415  batches / s\n",
      "batch:  2600    loss:  0.0889417    speed:  4.5175335722713506  batches / s\n",
      "batch:  2700    loss:  0.0963712    speed:  4.55207615520747  batches / s\n",
      "batch:  2800    loss:  0.0934391    speed:  4.481580287707286  batches / s\n",
      "batch:  2900    loss:  0.0926738    speed:  4.137033799996509  batches / s\n",
      "batch:  3000    loss:  0.0903339    speed:  4.367639631409596  batches / s\n",
      "batch:  3100    loss:  0.0914178    speed:  4.303695151205536  batches / s\n",
      "batch:  3200    loss:  0.0915134    speed:  4.249821242311449  batches / s\n",
      "batch:  3300    loss:  0.0880019    speed:  4.112231467536192  batches / s\n",
      "batch:  3400    loss:  0.0885496    speed:  3.945183271897091  batches / s\n",
      "batch:  3500    loss:  0.0826119    speed:  4.552597077611671  batches / s\n",
      "batch:  3600    loss:  0.0880156    speed:  4.532503845108791  batches / s\n",
      "batch:  3700    loss:  0.0886294    speed:  3.915976444149264  batches / s\n",
      "batch:  3800    loss:  0.0823093    speed:  3.4313570332055447  batches / s\n",
      "batch:  3900    loss:  0.0873085    speed:  3.722167984228097  batches / s\n",
      "batch:  4000    loss:  0.0892162    speed:  3.28252002194042  batches / s\n",
      "batch:  4100    loss:  0.0900242    speed:  3.244506162776664  batches / s\n",
      "batch:  4200    loss:  0.0874108    speed:  4.313036858460287  batches / s\n",
      "batch:  4300    loss:  0.0884529    speed:  3.017970264210478  batches / s\n",
      "batch:  4400    loss:  0.088211    speed:  3.6580750462236757  batches / s\n",
      "batch:  4500    loss:  0.0869644    speed:  4.29146535041255  batches / s\n",
      "batch:  4600    loss:  0.0842178    speed:  3.488015004595796  batches / s\n",
      "batch:  4700    loss:  0.0807313    speed:  3.0985424505082135  batches / s\n",
      "batch:  4800    loss:  0.0818782    speed:  4.035628145840725  batches / s\n",
      "batch:  4900    loss:  0.0807625    speed:  4.03392257995989  batches / s\n",
      "batch:  5000    loss:  0.0811945    speed:  3.9151413820397627  batches / s\n",
      "batch:  5100    loss:  0.082234    speed:  3.968559854214346  batches / s\n",
      "batch:  5200    loss:  0.0841963    speed:  4.05805520238543  batches / s\n",
      "batch:  5300    loss:  0.0814169    speed:  3.182307253486711  batches / s\n",
      "batch:  5400    loss:  0.0819968    speed:  4.02885151733427  batches / s\n",
      "batch:  5500    loss:  0.0835076    speed:  4.065205394250935  batches / s\n",
      "batch:  5600    loss:  0.0808411    speed:  4.041569219296341  batches / s\n",
      "batch:  5700    loss:  0.0827759    speed:  3.8683870524564146  batches / s\n",
      "batch:  5800    loss:  0.08214    speed:  4.521137236913326  batches / s\n",
      "batch:  5900    loss:  0.0827689    speed:  4.542076538427027  batches / s\n",
      "batch:  6000    loss:  0.0818344    speed:  4.392750869346069  batches / s\n",
      "batch:  6100    loss:  0.082646    speed:  4.403522810689403  batches / s\n",
      "batch:  6200    loss:  0.0791369    speed:  4.479994073250028  batches / s\n",
      "batch:  6300    loss:  0.0820203    speed:  4.4894544749113665  batches / s\n",
      "batch:  6400    loss:  0.0820534    speed:  4.322640828753228  batches / s\n",
      "batch:  6500    loss:  0.0803903    speed:  3.911240230499519  batches / s\n",
      "batch:  6600    loss:  0.082496    speed:  3.77320618999899  batches / s\n",
      "batch:  6700    loss:  0.0833815    speed:  3.0787618660454545  batches / s\n",
      "batch:  6800    loss:  0.0828131    speed:  3.244860357838833  batches / s\n",
      "batch:  6900    loss:  0.0810133    speed:  3.190067352241603  batches / s\n",
      "batch:  7000    loss:  0.0851445    speed:  3.2471722674922012  batches / s\n",
      "batch:  7100    loss:  0.080789    speed:  3.257100294038772  batches / s\n",
      "batch:  7200    loss:  0.0802237    speed:  3.799119856460561  batches / s\n",
      "batch:  7300    loss:  0.0817637    speed:  3.827063851456011  batches / s\n",
      "batch:  7400    loss:  0.0826005    speed:  3.827728944410319  batches / s\n",
      "batch:  7500    loss:  0.0826545    speed:  3.79282436258538  batches / s\n",
      "batch:  7600    loss:  0.0836424    speed:  3.758614367967413  batches / s\n",
      "batch:  7700    loss:  0.0853811    speed:  3.7972941613763105  batches / s\n",
      "batch:  7800    loss:  0.0842242    speed:  3.645194806896852  batches / s\n",
      "batch:  7900    loss:  0.085463    speed:  3.824842741644444  batches / s\n",
      "batch:  8000    loss:  0.0844831    speed:  3.7170047572440095  batches / s\n",
      "batch:  8100    loss:  0.084055    speed:  3.7439408945097568  batches / s\n",
      "batch:  8200    loss:  0.0853417    speed:  3.809413229763555  batches / s\n",
      "batch:  8300    loss:  0.084429    speed:  3.6408797234798627  batches / s\n",
      "batch:  8400    loss:  0.0835848    speed:  3.7895451803831  batches / s\n",
      "batch:  8500    loss:  0.0832907    speed:  3.855991456021739  batches / s\n",
      "batch:  8600    loss:  0.0850252    speed:  3.8356218100072  batches / s\n",
      "batch:  8700    loss:  0.085063    speed:  3.8527638793980143  batches / s\n",
      "batch:  8800    loss:  0.0835131    speed:  3.8288013752598142  batches / s\n",
      "batch:  8900    loss:  0.0843987    speed:  3.8397918745621213  batches / s\n",
      "batch:  9000    loss:  0.0828184    speed:  3.8161493229428047  batches / s\n",
      "batch:  9100    loss:  0.08421    speed:  3.7885831184156236  batches / s\n",
      "batch:  9200    loss:  0.0831952    speed:  3.7775612177019973  batches / s\n",
      "batch:  9300    loss:  0.0858666    speed:  3.861799722725567  batches / s\n",
      "batch:  9400    loss:  0.0857552    speed:  3.833995633821453  batches / s\n",
      "batch:  9500    loss:  0.0838256    speed:  3.866632136844003  batches / s\n",
      "batch:  9600    loss:  0.0837292    speed:  3.8458922383544616  batches / s\n",
      "batch:  9700    loss:  0.0829005    speed:  3.823124559253671  batches / s\n",
      "batch:  9800    loss:  0.0837977    speed:  3.86772965518554  batches / s\n",
      "batch:  9900    loss:  0.0855839    speed:  4.079385943826453  batches / s\n",
      "batch:  10000    loss:  0.0845712    speed:  4.057095544097848  batches / s\n",
      "batch:  10100    loss:  0.085084    speed:  4.042160319309448  batches / s\n",
      "batch:  10200    loss:  0.084751    speed:  2.8504823884490134  batches / s\n",
      "batch:  10300    loss:  0.0850367    speed:  3.865754314003689  batches / s\n",
      "batch:  10400    loss:  0.0838907    speed:  3.71258374737134  batches / s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  10500    loss:  0.0851422    speed:  3.1659914135689435  batches / s\n",
      "batch:  10600    loss:  0.0844953    speed:  3.245289230372149  batches / s\n",
      "batch:  10700    loss:  0.0854549    speed:  2.8369855059351603  batches / s\n",
      "batch:  10800    loss:  0.0848452    speed:  2.9510359277991682  batches / s\n",
      "batch:  10900    loss:  0.0731993    speed:  3.2203328225347625  batches / s\n",
      "batch:  11000    loss:  0.0791754    speed:  3.265002805137839  batches / s\n",
      "batch:  11100    loss:  0.0810935    speed:  3.245664015461922  batches / s\n",
      "batch:  11200    loss:  0.0821387    speed:  3.26719808769504  batches / s\n",
      "batch:  11300    loss:  0.0473684    speed:  3.2722085290229397  batches / s\n",
      "batch:  11400    loss:  0.0720018    speed:  3.249003329692684  batches / s\n",
      "batch:  11500    loss:  0.0780511    speed:  4.6650005494936755  batches / s\n",
      "batch:  11600    loss:  0.0817832    speed:  4.7775092170825655  batches / s\n",
      "batch:  11700    loss:  0.0838093    speed:  4.714796347307105  batches / s\n",
      "batch:  11800    loss:  0.0834922    speed:  4.066858811979589  batches / s\n",
      "batch:  11900    loss:  0.0841663    speed:  3.2471947923353826  batches / s\n",
      "batch:  12000    loss:  0.0873359    speed:  3.264052497973267  batches / s\n",
      "batch:  12100    loss:  0.0866542    speed:  4.455436462767324  batches / s\n",
      "batch:  12200    loss:  0.0847629    speed:  4.201417041064693  batches / s\n",
      "batch:  12300    loss:  0.0855895    speed:  4.552408270846272  batches / s\n",
      "batch:  12400    loss:  0.0855469    speed:  4.525039303709716  batches / s\n",
      "batch:  12500    loss:  0.084397    speed:  4.552035990385416  batches / s\n",
      "batch:  12600    loss:  0.0850059    speed:  4.444345970922839  batches / s\n",
      "batch:  12700    loss:  0.0851727    speed:  4.433144397901446  batches / s\n",
      "batch:  12800    loss:  0.0849434    speed:  4.067020138212617  batches / s\n",
      "batch:  12900    loss:  0.0840582    speed:  3.602157843618439  batches / s\n",
      "batch:  13000    loss:  0.0844447    speed:  3.685300923780163  batches / s\n",
      "batch:  13100    loss:  0.0839101    speed:  3.7084321539567187  batches / s\n",
      "batch:  13200    loss:  0.0847745    speed:  3.840803863557649  batches / s\n",
      "batch:  13300    loss:  0.0846126    speed:  3.8483010583152706  batches / s\n",
      "batch:  13400    loss:  0.0848088    speed:  3.6805942579344446  batches / s\n",
      "batch:  13500    loss:  0.0839333    speed:  3.863165754981725  batches / s\n",
      "batch:  13600    loss:  0.0832022    speed:  3.837658604291295  batches / s\n",
      "batch:  13700    loss:  0.0843351    speed:  3.8608698940371124  batches / s\n",
      "batch:  13800    loss:  0.0846576    speed:  3.7808943907501904  batches / s\n",
      "batch:  13900    loss:  0.0837046    speed:  3.8283094262667285  batches / s\n",
      "batch:  14000    loss:  0.0836182    speed:  3.801222404096917  batches / s\n",
      "batch:  14100    loss:  0.0844755    speed:  3.6340995895870543  batches / s\n",
      "batch:  14200    loss:  0.0837377    speed:  3.8479248133657786  batches / s\n",
      "batch:  14300    loss:  0.0831343    speed:  3.722354094606011  batches / s\n",
      "batch:  14400    loss:  0.0839803    speed:  3.881042905178485  batches / s\n",
      "batch:  14500    loss:  0.0839382    speed:  3.863662895397638  batches / s\n",
      "batch:  14600    loss:  0.0838502    speed:  3.746533289863358  batches / s\n",
      "batch:  14700    loss:  0.0834718    speed:  4.658014980973932  batches / s\n",
      "batch:  14800    loss:  0.083904    speed:  4.7953136557787595  batches / s\n",
      "batch:  14900    loss:  0.0830364    speed:  5.136652359787576  batches / s\n",
      "batch:  15000    loss:  0.0838016    speed:  5.159250865138986  batches / s\n",
      "batch:  15100    loss:  0.0834549    speed:  5.001480540809505  batches / s\n",
      "batch:  15200    loss:  0.0831245    speed:  4.864182196043206  batches / s\n",
      "batch:  15300    loss:  0.0834703    speed:  4.766007904545485  batches / s\n",
      "batch:  15400    loss:  0.0841756    speed:  4.567329293031719  batches / s\n",
      "batch:  15500    loss:  0.0837515    speed:  4.27534434923619  batches / s\n",
      "batch:  15600    loss:  0.0841447    speed:  4.060933639733129  batches / s\n",
      "batch:  15700    loss:  0.0836544    speed:  4.129343155561422  batches / s\n",
      "batch:  15800    loss:  0.0833639    speed:  4.0829932740433  batches / s\n",
      "batch:  15900    loss:  0.0840416    speed:  4.124590988038373  batches / s\n",
      "batch:  16000    loss:  0.0842436    speed:  4.118708321354037  batches / s\n",
      "batch:  16100    loss:  0.0833476    speed:  4.087071425273666  batches / s\n",
      "batch:  16200    loss:  0.0835573    speed:  4.046057504992043  batches / s\n",
      "batch:  16300    loss:  0.0831806    speed:  4.069800302853091  batches / s\n",
      "batch:  16400    loss:  0.0829155    speed:  4.031979993292633  batches / s\n",
      "batch:  16500    loss:  0.0874203    speed:  4.03498018467114  batches / s\n",
      "batch:  16600    loss:  0.0865855    speed:  4.102938092354825  batches / s\n",
      "batch:  16700    loss:  0.0844811    speed:  4.1195429870212665  batches / s\n",
      "batch:  16800    loss:  0.0837277    speed:  4.094959624320446  batches / s\n",
      "batch:  16900    loss:  0.0831107    speed:  4.094080819653271  batches / s\n",
      "batch:  17000    loss:  0.0834254    speed:  4.121286143081489  batches / s\n",
      "batch:  17100    loss:  0.0838205    speed:  4.127648178078518  batches / s\n",
      "batch:  17200    loss:  0.0835383    speed:  4.0064341304954425  batches / s\n",
      "batch:  17300    loss:  0.0829764    speed:  3.7669041266829586  batches / s\n",
      "batch:  17400    loss:  0.0839189    speed:  4.053166578909182  batches / s\n",
      "batch:  17500    loss:  0.0835518    speed:  3.9488450921884466  batches / s\n",
      "batch:  17600    loss:  0.0832111    speed:  4.13850650329914  batches / s\n",
      "batch:  17700    loss:  0.0837277    speed:  4.127729380088424  batches / s\n",
      "batch:  17800    loss:  0.0834966    speed:  4.10400898957444  batches / s\n",
      "batch:  17900    loss:  0.083452    speed:  4.133132970731893  batches / s\n",
      "batch:  18000    loss:  0.0840115    speed:  4.133233735598365  batches / s\n",
      "batch:  18100    loss:  0.0843015    speed:  4.133511699404749  batches / s\n",
      "batch:  18200    loss:  0.0838344    speed:  4.122910648483202  batches / s\n",
      "batch:  18300    loss:  0.0837106    speed:  4.077654417742021  batches / s\n",
      "batch:  18400    loss:  0.0831903    speed:  4.027544951211279  batches / s\n",
      "batch:  18500    loss:  0.0834745    speed:  4.0318753456801355  batches / s\n",
      "batch:  18600    loss:  0.0836252    speed:  4.082305380366881  batches / s\n",
      "batch:  18700    loss:  0.084097    speed:  4.132394368472948  batches / s\n",
      "batch:  18800    loss:  0.0839315    speed:  3.946564083259993  batches / s\n",
      "batch:  18900    loss:  0.084496    speed:  4.064170756131838  batches / s\n",
      "batch:  19000    loss:  0.0843601    speed:  3.5611441145414595  batches / s\n",
      "batch:  19100    loss:  0.0833637    speed:  4.0029695796743745  batches / s\n",
      "batch:  19200    loss:  0.0832414    speed:  3.950461525321819  batches / s\n",
      "batch:  19300    loss:  0.0835672    speed:  3.9032349311712458  batches / s\n",
      "batch:  19400    loss:  0.0835036    speed:  3.7632168926727045  batches / s\n",
      "batch:  19500    loss:  0.0829759    speed:  4.006241757485186  batches / s\n",
      "batch:  19600    loss:  0.083189    speed:  4.037179361197997  batches / s\n",
      "batch:  19700    loss:  0.0836724    speed:  4.071383200525813  batches / s\n",
      "batch:  19800    loss:  0.0830755    speed:  4.072007801611169  batches / s\n",
      "batch:  19900    loss:  0.0833473    speed:  4.057885164387729  batches / s\n",
      "batch:  20000    loss:  0.0829887    speed:  4.04377289599953  batches / s\n",
      "batch:  20100    loss:  0.0827869    speed:  3.987652256024733  batches / s\n",
      "batch:  20200    loss:  0.0830243    speed:  3.982063972338339  batches / s\n",
      "batch:  20300    loss:  0.082949    speed:  4.023091060794763  batches / s\n",
      "batch:  20400    loss:  0.0842103    speed:  4.017332239160565  batches / s\n",
      "batch:  20500    loss:  0.0838898    speed:  4.001948466062907  batches / s\n",
      "batch:  20600    loss:  0.0831195    speed:  3.8113515540595535  batches / s\n",
      "batch:  20700    loss:  0.0831935    speed:  3.8712804850471785  batches / s\n",
      "batch:  20800    loss:  0.0833203    speed:  3.8558617500350074  batches / s\n",
      "batch:  20900    loss:  0.083334    speed:  3.804585886328701  batches / s\n",
      "batch:  21000    loss:  0.0892543    speed:  3.7023166860675536  batches / s\n",
      "batch:  21100    loss:  0.0875584    speed:  3.8500386279405454  batches / s\n",
      "batch:  21200    loss:  0.0860584    speed:  3.7602477366433043  batches / s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  21300    loss:  0.0850408    speed:  3.825056005330833  batches / s\n",
      "batch:  21400    loss:  0.0841928    speed:  3.8482878177066246  batches / s\n",
      "batch:  21500    loss:  0.0894734    speed:  3.8516525183335104  batches / s\n",
      "batch:  21600    loss:  0.0877128    speed:  3.7010750190681114  batches / s\n",
      "batch:  21700    loss:  0.0864099    speed:  3.8499441656282714  batches / s\n",
      "batch:  21800    loss:  0.0850193    speed:  3.8340573164650085  batches / s\n",
      "batch:  21900    loss:  0.084479    speed:  3.76318501938151  batches / s\n",
      "batch:  22000    loss:  0.0841907    speed:  3.246365149190743  batches / s\n",
      "batch:  22100    loss:  0.0843619    speed:  3.783037917886031  batches / s\n",
      "batch:  22200    loss:  0.0838551    speed:  3.537359764688336  batches / s\n",
      "batch:  22300    loss:  0.0591513    speed:  3.93634226524827  batches / s\n",
      "batch:  22400    loss:  0.066007    speed:  4.663147686948429  batches / s\n",
      "batch:  22500    loss:  0.0708234    speed:  4.107608164855258  batches / s\n",
      "batch:  22600    loss:  0.0739209    speed:  4.718538538958353  batches / s\n",
      "batch:  22700    loss:  0.076002    speed:  4.233694505362619  batches / s\n",
      "batch:  22800    loss:  0.077632    speed:  3.8695446131645963  batches / s\n",
      "batch:  22900    loss:  0.078927    speed:  3.998792783886273  batches / s\n",
      "batch:  23000    loss:  0.0801218    speed:  3.747532539475124  batches / s\n",
      "batch:  23100    loss:  0.0809345    speed:  3.9201821651574544  batches / s\n",
      "batch:  23200    loss:  0.0816644    speed:  3.8936297352235902  batches / s\n",
      "batch:  23300    loss:  0.0822404    speed:  4.120008261815426  batches / s\n",
      "batch:  23400    loss:  0.0825617    speed:  4.1134283226457  batches / s\n",
      "batch:  23500    loss:  0.0828095    speed:  3.8842811128879386  batches / s\n",
      "batch:  23600    loss:  0.0828395    speed:  3.9983119843174575  batches / s\n",
      "batch:  23700    loss:  0.0825553    speed:  3.9701137038988223  batches / s\n",
      "batch:  23800    loss:  0.0825055    speed:  3.9117461736424364  batches / s\n",
      "batch:  23900    loss:  0.0827119    speed:  3.9927093119434565  batches / s\n",
      "batch:  24000    loss:  0.08326    speed:  4.078754436734012  batches / s\n",
      "batch:  24100    loss:  0.0834799    speed:  4.082335537917005  batches / s\n",
      "batch:  24200    loss:  0.0834875    speed:  3.101590567304725  batches / s\n",
      "batch:  24300    loss:  0.0831241    speed:  4.210748784625942  batches / s\n",
      "batch:  24400    loss:  0.0828601    speed:  4.462189066397167  batches / s\n",
      "batch:  24500    loss:  0.0834325    speed:  4.523768474692919  batches / s\n",
      "batch:  24600    loss:  0.083342    speed:  4.521958609202409  batches / s\n",
      "batch:  24700    loss:  0.0835727    speed:  4.5388112576479624  batches / s\n",
      "batch:  24800    loss:  0.083151    speed:  4.531238708847864  batches / s\n",
      "batch:  24900    loss:  0.0831081    speed:  4.434578369565586  batches / s\n",
      "batch:  25000    loss:  0.0829513    speed:  4.12028910426967  batches / s\n",
      "batch:  25100    loss:  0.08041    speed:  4.469211834365326  batches / s\n",
      "batch:  25200    loss:  0.0808347    speed:  4.567428666323645  batches / s\n",
      "batch:  25300    loss:  0.0810924    speed:  4.463233406325783  batches / s\n",
      "batch:  25400    loss:  0.0813089    speed:  4.568164750193248  batches / s\n",
      "batch:  25500    loss:  0.0817573    speed:  4.525166626050288  batches / s\n",
      "batch:  25600    loss:  0.0821275    speed:  4.548439199203923  batches / s\n",
      "batch:  25700    loss:  0.0821874    speed:  4.562807116588559  batches / s\n",
      "batch:  25800    loss:  0.0824794    speed:  4.538810373557383  batches / s\n",
      "batch:  25900    loss:  0.0827978    speed:  4.496424470641479  batches / s\n",
      "batch:  26000    loss:  0.0829071    speed:  4.5467692326404405  batches / s\n",
      "batch:  26100    loss:  0.0828229    speed:  4.541397664660779  batches / s\n",
      "batch:  26200    loss:  0.0831    speed:  4.495895263223038  batches / s\n",
      "batch:  26300    loss:  0.0827777    speed:  4.549395856577382  batches / s\n",
      "batch:  26400    loss:  0.083    speed:  4.556316564356131  batches / s\n",
      "batch:  26500    loss:  0.0829992    speed:  4.305665517665628  batches / s\n",
      "batch:  26600    loss:  0.0839102    speed:  4.538088186623216  batches / s\n",
      "batch:  26700    loss:  0.0835628    speed:  4.800585221488659  batches / s\n",
      "batch:  26800    loss:  0.0830749    speed:  4.794770845713068  batches / s\n",
      "batch:  26900    loss:  0.0831512    speed:  4.810153702630717  batches / s\n",
      "batch:  27000    loss:  0.0833938    speed:  4.799684401855336  batches / s\n",
      "batch:  27100    loss:  0.0835801    speed:  4.794216541031444  batches / s\n",
      "batch:  27200    loss:  0.083032    speed:  4.695078068933236  batches / s\n",
      "batch:  27300    loss:  0.0830564    speed:  4.713234513460894  batches / s\n",
      "batch:  27400    loss:  0.0834488    speed:  3.730482182617544  batches / s\n",
      "batch:  27500    loss:  0.0831091    speed:  2.863121840827971  batches / s\n",
      "batch:  27600    loss:  0.0833211    speed:  3.671055661318564  batches / s\n",
      "batch:  27700    loss:  0.0837192    speed:  4.2188963688235095  batches / s\n",
      "batch:  27800    loss:  0.0832876    speed:  4.245942988791969  batches / s\n",
      "batch:  27900    loss:  0.0834541    speed:  3.2765105919628694  batches / s\n",
      "batch:  28000    loss:  0.083395    speed:  3.253152327426883  batches / s\n",
      "batch:  28100    loss:  0.0832456    speed:  3.2637259727147696  batches / s\n",
      "batch:  28200    loss:  0.0833202    speed:  3.2830519825774154  batches / s\n",
      "batch:  28300    loss:  0.0833646    speed:  3.2565261908992267  batches / s\n",
      "batch:  28400    loss:  0.083274    speed:  3.2796402453190145  batches / s\n",
      "batch:  28500    loss:  0.0830808    speed:  3.2591289518989144  batches / s\n",
      "batch:  28600    loss:  0.0829736    speed:  3.2627069666665673  batches / s\n",
      "batch:  28700    loss:  0.0832407    speed:  3.272582101877538  batches / s\n",
      "batch:  28800    loss:  0.0832885    speed:  3.265502738159934  batches / s\n",
      "batch:  28900    loss:  0.0834463    speed:  3.2836540377722647  batches / s\n",
      "batch:  29000    loss:  0.0833977    speed:  3.849382611673518  batches / s\n",
      "batch:  29100    loss:  0.0834582    speed:  3.856924468942569  batches / s\n",
      "batch:  29200    loss:  0.0832466    speed:  3.8630538893339224  batches / s\n",
      "batch:  29300    loss:  0.0833497    speed:  3.846608764513052  batches / s\n",
      "batch:  29400    loss:  0.0833771    speed:  3.8066184491826  batches / s\n",
      "batch:  29500    loss:  0.0832078    speed:  3.020661106373996  batches / s\n",
      "batch:  29600    loss:  0.0833779    speed:  2.882658911515411  batches / s\n",
      "batch:  29700    loss:  0.0832292    speed:  2.510959387678745  batches / s\n",
      "batch:  29800    loss:  0.0831698    speed:  3.533402270401078  batches / s\n",
      "batch:  29900    loss:  0.0829041    speed:  3.792969962099365  batches / s\n",
      "Loss:  0.086614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harlinton\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Requested projection is different from current axis projection, creating new axis with requested projection.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED:\n",
      "[[ 0.02579736  0.01654485  0.03081695  0.03393485  0.01695598  0.03086287\n",
      "   0.0298661   0.01551705  0.02976091  0.0298661   0.02045051  0.03196482\n",
      "   0.02665394  0.02497284  0.03044963  0.03307827  0.0251784   0.03031189\n",
      "   0.03008025  0.02476728  0.03178116  0.02451249  0.02230055  0.03072512\n",
      "   0.03521972  0.02271168  0.03090878  0.02494078  0.02867294  0.03017415\n",
      "   0.03479143  0.02908406  0.03012823  0.02472663  0.03011186  0.03031189\n",
      "   0.03479143  0.03052298  0.03008232  0.02729637  0.03751204  0.03113836\n",
      "   0.03286413  0.03751204  0.03113836  0.02686808  0.04594003  0.03214848\n",
      "   0.03307827  0.04635115  0.03155159  0.0260115   0.04799564  0.03022006\n",
      "   0.03286413  0.0482012   0.02962317  0.03008025  0.00955579  0.02934768]]\n"
     ]
    }
   ],
   "source": [
    "class ModelNetwork:\n",
    "    \"\"\"\n",
    "    RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "    The network allows for a dynamic number of iterations, depending on the\n",
    "    inputs it receives.\n",
    "       out   (fc layer; out_size)\n",
    "        ^\n",
    "       lstm\n",
    "        ^\n",
    "       lstm  (lstm size)\n",
    "        ^\n",
    "        in   (in_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session, learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.session = session\n",
    "\n",
    "        self.learning_rate = tf.constant( learning_rate )\n",
    "\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            ## (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(tf.float32, shape=(None, None, self.in_size), name=\"xinput\")\n",
    "            self.lstm_init_value = tf.placeholder(tf.float32, shape=(None, self.num_layers*2*self.lstm_size), name=\"lstm_init_value\")\n",
    "\n",
    "            # LSTM\n",
    "            self.lstm_cells = [ tf.contrib.rnn.BasicLSTMCell(self.lstm_size, forget_bias=1.0, state_is_tuple=False) for i in range(self.num_layers)]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cells, state_is_tuple=False)\n",
    "\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(self.lstm, self.xinput, initial_state=self.lstm_init_value, dtype=tf.float32)\n",
    "\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(tf.random_normal( (self.lstm_size, self.out_size), stddev=0.01 ))\n",
    "            self.rnn_out_B = tf.Variable(tf.random_normal( (self.out_size, ), stddev=0.01 ))\n",
    "\n",
    "            outputs_reshaped = tf.reshape( outputs, [-1, self.lstm_size] )\n",
    "            network_output = ( tf.matmul( outputs_reshaped, self.rnn_out_W ) + self.rnn_out_B )\n",
    "\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape( tf.nn.softmax( network_output), (batch_time_shape[0], batch_time_shape[1], self.out_size) )\n",
    "\n",
    "\n",
    "            ## Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(tf.float32, (None, None, self.out_size))\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "\n",
    "            self.cost = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(y_batch_long, network_output))), 'cost')\n",
    "            tf.summary.scalar('cost', self.cost)\n",
    "            \n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.learning_rate, 0.9).minimize(self.cost)\n",
    "\n",
    "\n",
    "    ## Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        ## Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "\n",
    "        out, next_lstm_state = self.session.run([self.final_outputs, self.lstm_new_state], feed_dict={self.xinput:[x], self.lstm_init_value:[init_value]   } )\n",
    "\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "\n",
    "        return out[0][0]\n",
    "\n",
    "    def train_batch(self, xbatch, ybatch, merged):\n",
    "        init_value = np.zeros((xbatch.shape[0], self.num_layers*2*self.lstm_size))\n",
    "\n",
    "        summary, cost, _ = self.session.run([merged, self.cost, self.train_op], feed_dict={self.xinput:xbatch, self.y_batch:ybatch, self.lstm_init_value:init_value   } )\n",
    "\n",
    "        return summary, cost\n",
    "\n",
    "###################################### Load the data ##############################################################\n",
    "actions = read_actions('./data', simplified=True, normed=True)\n",
    "\n",
    "in_size = out_size = 20*3 # frame (x y z for 20 joints)\n",
    "lstm_size = 256 #128\n",
    "num_layers = 2\n",
    "batch_size = 1 #128\n",
    "time_steps = 10 #50\n",
    "\n",
    "NUM_TRAIN_BATCHES = 30000\n",
    "\n",
    "LEN_TEST_TEXT = 200 # Number of test characters of text to generate after training the network\n",
    "\n",
    "## Initialize the network\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "net = ModelNetwork(in_size = in_size,\n",
    "                    lstm_size = lstm_size,\n",
    "                    num_layers = num_layers,\n",
    "                    out_size = out_size,\n",
    "                    session = sess,\n",
    "                    learning_rate = 0.003,\n",
    "                    name = \"msr_rnn_network\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "ckpt_file = \"./Training_model/model.ckpt\"\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "################################################# 1) TRAIN THE NETWORK #########################################################################\n",
    "if True:\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "    last_time = time.time()\n",
    "\n",
    "    for i in range(NUM_TRAIN_BATCHES):\n",
    "\n",
    "        # data = random.choice(actions)\n",
    "        data = actions[0]\n",
    "        time_steps = len(data) - 1\n",
    "\n",
    "        batch = np.zeros((batch_size, time_steps, in_size))\n",
    "        batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "\n",
    "        possible_batch_ids = range(len(data)-time_steps-1)\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        # batch_id = random.sample( possible_batch_ids, batch_size )\n",
    "        batch_id = [0]\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k+j for k in batch_id]\n",
    "            ind2 = [k+j+1 for k in batch_id]\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "        \n",
    "        summary, cst = net.train_batch(batch, batch_y, merged)\n",
    "        train_writer.add_summary(summary, i)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (i%100) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "            print(\"batch: \",i,\"   loss: \",cst,\"   speed: \",(100.0/diff),\" batches / s\")\n",
    "\n",
    "            saver.save(sess, ckpt_file)\n",
    "        \n",
    "        \n",
    "    print (\"Loss: \" , cst)\n",
    "############################### GENERATE LEN_TEST_TEXT CHARACTERS USING THE TRAINED NETWORK########################################\n",
    "\n",
    "TEST_PREFIX = actions[0][:1]\n",
    "for i in range(len(TEST_PREFIX)):\n",
    "    out = net.run_step( [TEST_PREFIX[i]] , i==0)\n",
    "    save_frame(1000*out, './skeleton_action/{0}.png'.format(i), True)\n",
    "\n",
    "print(\"PRED:\")\n",
    "gen_str = TEST_PREFIX\n",
    "for j in range(LEN_TEST_TEXT):   \n",
    "    out = net.run_step( [out] , False )\n",
    "    save_frame(1000*out, './skeleton_action/{0}.png'.format(i+j), True)\n",
    "\n",
    "print(gen_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center>Results and configuration</center>\n",
    "\n",
    "<b>MSR Action3D Dataset : </b>\n",
    "20 action types, 10 subjects, each subject performs each action 2 or 3 times. There are 567 depth map sequences in total. The resolution is 640x240. The data was recorded with a depth sensor similar to the Kinect device\n",
    "\n",
    "<b>Parameters used: </b>\n",
    "\n",
    "\n",
    "\n",
    "<b>Number of test characters:</b>  200\n",
    "\n",
    "<b>LSTM_size:</b> 256\n",
    "\n",
    "<b>Learning rate:</b> 0.003\n",
    "\n",
    "<b>Numero of layers: </b> 2\n",
    "\n",
    "<b>Loss function:</b> Euclidean Distance\n",
    "\n",
    "<b>Optimizer:</b> RMSProp\n",
    "\n",
    "<b>Input size</b> = Output size= 60 (20*3)(x y z for 20 joints)\n",
    "\n",
    "<b> NUM_TRAIN_BATCHES: </b> = 30000\n",
    "\n",
    "##  After training and evaluation of the neural network with the parameters established in this homework, we obtained:\n",
    "\n",
    "<b>Loss:</b>  0.086614 and the neural network generated 200 frames with which I created the following video: https://www.youtube.com/watch?v=-0TdVNibr6k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "framerate = 10 \n",
    "from os.path import isfile, join\n",
    " \n",
    "def convert_frames_to_video(pathIn,pathOut,fps):\n",
    "    frame_array = []\n",
    "    files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n",
    " \n",
    " \n",
    "    for i in range(len(files)):\n",
    "        filename=pathIn + files[i]\n",
    "        #reading each files\n",
    "        img = cv2.imread(filename)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        print(filename)\n",
    "        #inserting the frames into an image array\n",
    "        frame_array.append(img)\n",
    "     \n",
    "    #out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Be sure to use lower case\n",
    "    out = cv2.VideoWriter(pathOut, fourcc, framerate, (size))\n",
    "    #out = cv2.VideoWriter(pathOut, -1, 1, (size))\n",
    " \n",
    "    for i in range(len(frame_array)):\n",
    "        # writing to a image array\n",
    "        out.write(frame_array[i])\n",
    "    out.release()\n",
    "pathIn = \"./skeleton_action/\"\n",
    "pathOut = 'Video.mp4'\n",
    "fps = 25.0\n",
    "convert_frames_to_video(pathIn, pathOut, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generated a vÃ­deo with the frame generated before \n",
    ":https://www.youtube.com/watch?v=-0TdVNibr6k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
